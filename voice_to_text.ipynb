{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "voice-to-text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKWFrO_N0tZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!git clone -b master https://github.com/WellsCui/speech-to-text.git\n",
        "!mv -f ./speech-to-text/* ./\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jmS9okHhk3F",
        "colab_type": "code",
        "outputId": "a09ab6f1-c27e-48b1-b288-e6a75a497543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "tags": []
      },
      "source": [
        "!python train.py -c config.json"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "use device: cpu\n",
            "loading train data ...\n",
            "remaining train data length: 0\n",
            "pushing new round train data: 0\n",
            "pushed new train data\n",
            "pushed new train data\n",
            "pushed new train data\n",
            "target_padded_chars_2 shape: torch.Size([171, 2])\n",
            "epoch 0, iter 1, avg. loss 753.04, avg. ppl 22313678724742.12 cum. examples 2, speed 0.75 words/sec, time elapsed 65.24 sec\n",
            "target_padded_chars_2 shape: torch.Size([171, 2])\n",
            "epoch 1, iter 2, avg. loss 561.88, avg. ppl 9121568811.91 cum. examples 4, speed 0.79 words/sec, time elapsed 127.00 sec\n",
            "target_padded_chars_2 shape: torch.Size([171, 2])\n",
            "epoch 2, iter 3, avg. loss 500.10, avg. ppl 732686851.78 cum. examples 6, speed 0.79 words/sec, time elapsed 188.83 sec\n",
            "target_padded_chars_2 shape: torch.Size([171, 2])\n",
            "^C\n",
            "Process Process-2:\n",
            "Traceback (most recent call last):\n",
            "  File \"/anaconda3/envs/tf2/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/anaconda3/envs/tf2/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Users/wui/projects/ai/voice-to-text/utils.py\", line 379, in batch_iter_to_queue2\n",
            "    loss_sum = loss_sum + loss_queue.get()\n",
            "  File \"/anaconda3/envs/tf2/lib/python3.7/multiprocessing/queues.py\", line 94, in get\n",
            "    res = self._recv_bytes()\n",
            "  File \"/anaconda3/envs/tf2/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/anaconda3/envs/tf2/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/anaconda3/envs/tf2/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAquc-ZX_ImV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8XcPs0fjeKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def evaluate_ppl(model, dev_data, batch_size=32):\n",
        "    \"\"\" Evaluate perplexity on dev sentences\n",
        "    @param model (NMT): NMT Model\n",
        "    @param dev_data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
        "    @param batch_size (batch size)\n",
        "    @returns ppl (perplixty on dev sentences)\n",
        "    \"\"\"\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    cum_loss = 0.\n",
        "    cum_tgt_words = 0.\n",
        "\n",
        "    # no_grad() signals backend to throw away all gradients\n",
        "    with torch.no_grad():\n",
        "        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n",
        "            loss = -model(src_sents, tgt_sents).sum()\n",
        "\n",
        "            cum_loss += loss.item()\n",
        "            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n",
        "            cum_tgt_words += tgt_word_num_to_predict\n",
        "\n",
        "        ppl = np.exp(cum_loss / cum_tgt_words)\n",
        "\n",
        "    if was_training:\n",
        "        model.train()\n",
        "\n",
        "    return ppl\n",
        "\n",
        "\n",
        "def compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n",
        "    \"\"\" Given decoding results and reference sentences, compute corpus-level BLEU score.\n",
        "    @param references (List[List[str]]): a list of gold-standard reference target sentences\n",
        "    @param hypotheses (List[Hypothesis]): a list of hypotheses, one for each reference\n",
        "    @returns bleu_score: corpus-level BLEU score\n",
        "    \"\"\"\n",
        "    if references[0][0] == '<s>':\n",
        "        references = [ref[1:-1] for ref in references]\n",
        "    bleu_score = corpus_bleu([[ref] for ref in references],\n",
        "                             [hyp.value for hyp in hypotheses])\n",
        "    return bleu_score\n",
        "\n",
        "\n",
        "def beam_search(model: NMT, test_data_src: List[List[float]], beam_size: int, max_decoding_time_step: int) -> List[List[Hypothesis]]:\n",
        "    \"\"\" Run beam search to construct hypotheses for a list of src-language sentences.\n",
        "    @param model (NMT): NMT Model\n",
        "    @param test_data_src (List[List[float]]): List of sentences (words) in source language, from test set.\n",
        "    @param beam_size (int): beam_size (# of hypotheses to hold for a translation at every step)\n",
        "    @param max_decoding_time_step (int): maximum sentence length that Beam search can produce\n",
        "    @returns hypotheses (List[List[Hypothesis]]): List of Hypothesis translations for every source sentence.\n",
        "    \"\"\"\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    hypotheses = []\n",
        "    with torch.no_grad():\n",
        "        for src_sent in tqdm(test_data_src, desc='Decoding', file=sys.stdout):\n",
        "            example_hyps = model.beam_search(src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step)\n",
        "\n",
        "            hypotheses.append(example_hyps)\n",
        "\n",
        "    if was_training: model.train(was_training)\n",
        "\n",
        "    return hypotheses\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tVM8cK4jlZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import IPython.display as ipd\n",
        "\n",
        "sample_rate = 22000\n",
        "resample_rate = 8000\n",
        "train_records = 80\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So0iWEVIfhes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "# load model\n",
        "# if os.path.isfile(model_save_path):\n",
        "#   params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n",
        "#   model.load_state_dict(params['state_dict'])\n",
        "#   model = model.to(device)\n",
        "\n",
        "#   print('restore parameters of the optimizers', file=sys.stderr)\n",
        "#   optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC1KZ7e9ldXO",
        "colab_type": "code",
        "outputId": "3f3b697a-f26e-47f7-be82-e5467284e486",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from multiprocessing import Process, Queue\n",
        "from utils import load_voices, load_voices_files, split_source_with_pad, read_corpus_from_LJSpeech, batch_iter, get_voice_files_and_corpus, batch_iter_to_queue, batch_iter_to_queue2, load_train_data\n",
        "\n",
        "dev_files, dev_corpus = get_voice_files_and_corpus('dataset/dev', 5)\n",
        "dev_data = list(zip(load_voices_files(dev_files, sample_rate, resample_rate), dev_corpus))\n",
        "\n",
        "\n",
        "clip_grad = 5.0\n",
        "valid_niter = 1000\n",
        "log_every = 10\n",
        "max_epoch = 4000\n",
        "\n",
        "num_trial = 0\n",
        "train_records = 16\n",
        "epoch_size = 16\n",
        "train_batch_size = 16\n",
        "\n",
        "train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n",
        "cum_examples = report_examples = epoch = valid_num = 0\n",
        "hist_valid_scores = []\n",
        "train_time = begin_time = time.time()\n",
        "print('begin Maximum Likelihood training')\n",
        "data_queue = Queue(1)\n",
        "batch_queue = Queue(1)\n",
        "loss_queue = Queue(2)\n",
        "\n",
        "train_data_to_queue_process = Process(target=load_train_data, args=('dataset/train', train_records, epoch_size, data_queue, 10))\n",
        "train_data_to_queue_process.start()\n",
        "\n",
        "batch_iter_to_queue_process = Process(target=batch_iter_to_queue2, args=(data_queue, batch_queue, loss_queue, max_epoch, train_batch_size, True))\n",
        "batch_iter_to_queue_process.start()\n",
        "epoch, voices, tgt_sents = batch_queue.get(True)\n",
        "current_epoch = -1\n",
        "train_losses = []\n",
        "while voices is not None and tgt_sents is not None:\n",
        "\n",
        "    train_iter += 1\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # voices = load_voices_files(voice_files, sample_rate, resample_rate)\n",
        "    # voices = voice_files\n",
        "    batch_size = len(voices)\n",
        "\n",
        "    example_losses = -model(voices, tgt_sents) # (batch_size,)\n",
        "    batch_loss = example_losses.sum()\n",
        "    loss = batch_loss / batch_size\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # clip gradient\n",
        "    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    batch_losses_val = batch_loss.item()\n",
        "    report_loss += batch_losses_val\n",
        "    cum_loss += batch_losses_val\n",
        "\n",
        "    tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n",
        "    report_tgt_words += tgt_words_num_to_predict\n",
        "    cum_tgt_words += tgt_words_num_to_predict\n",
        "    report_examples += batch_size\n",
        "    cum_examples += batch_size\n",
        "    loss_queue.put(report_loss / report_examples)\n",
        "    train_losses.append({'epoch': epoch,\n",
        "                         'iter': train_iter,\n",
        "                         'loss': report_loss / report_examples,\n",
        "                         'ppl': math.exp(report_loss / report_tgt_words),\n",
        "                         'cum': cum_examples,\n",
        "                         'speed': report_tgt_words / (time.time() - train_time)})\n",
        "\n",
        "    if train_iter % log_every == 0:\n",
        "        print('epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f ' \\\n",
        "              'cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec' % (epoch, train_iter,\n",
        "                                                                                  report_loss / report_examples,\n",
        "                                                                                  math.exp(report_loss / report_tgt_words),\n",
        "                                                                                  cum_examples,\n",
        "                                                                                  report_tgt_words / (time.time() - train_time),\n",
        "                                                                                  time.time() - begin_time), file=sys.stderr)\n",
        "\n",
        "        train_time = time.time()\n",
        "        report_loss = report_tgt_words = report_examples = 0.\n",
        "\n",
        "    # perform validation\n",
        "    if train_iter % valid_niter == 0:\n",
        "        print('epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d' % (epoch, train_iter,\n",
        "                                                                                  cum_loss / cum_examples,\n",
        "                                                                                  np.exp(cum_loss / cum_tgt_words),\n",
        "                                                                                  cum_examples), file=sys.stderr)\n",
        "\n",
        "        cum_loss = cum_examples = cum_tgt_words = 0.\n",
        "        valid_num += 1\n",
        "\n",
        "        print('begin validation ...', file=sys.stderr)\n",
        "\n",
        "        # compute dev. ppl and bleu\n",
        "        dev_ppl = evaluate_ppl(model, dev_data, batch_size=128)   # dev batch size can be a bit larger\n",
        "        valid_metric = -dev_ppl\n",
        "\n",
        "        print('validation: iter %d, dev. ppl %f' % (train_iter, dev_ppl), file=sys.stderr)\n",
        "\n",
        "        is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n",
        "        hist_valid_scores.append(valid_metric)\n",
        "\n",
        "        if is_better:\n",
        "            patience = 0\n",
        "            # print('save currently the best model to [%s]' % model_save_path, file=sys.stderr)\n",
        "            # model.save(model_save_path)\n",
        "\n",
        "            # # also save the optimizers' state\n",
        "            # torch.save(optimizer.state_dict(), model_save_path + '.optim')\n",
        "        elif patience < 100:\n",
        "            patience += 1\n",
        "            print('hit patience %d' % patience, file=sys.stderr)\n",
        "\n",
        "            if patience == 100:\n",
        "                num_trial += 1\n",
        "                print('hit #%d trial' % num_trial, file=sys.stderr)\n",
        "                if num_trial == 3:\n",
        "                    print('early stop!', file=sys.stderr)\n",
        "                    exit(0)\n",
        "\n",
        "                # decay lr, and restore from previously best checkpoint\n",
        "                lr = optimizer.param_groups[0]['lr'] * 0.5\n",
        "                print('load previously best model and decay learning rate to %f' % lr, file=sys.stderr)\n",
        "\n",
        "                # load model\n",
        "                params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n",
        "                model.load_state_dict(params['state_dict'])\n",
        "                model = model.to(device)\n",
        "\n",
        "                print('restore parameters of the optimizers', file=sys.stderr)\n",
        "                optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n",
        "\n",
        "                # set new lr\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = lr\n",
        "\n",
        "                # reset patience\n",
        "                patience = 0\n",
        "        \n",
        "    epoch, voices, tgt_sents = batch_queue.get()\n",
        "batch_iter_to_queue_process.join()\n",
        "      \n",
        "\n",
        "\n",
        "        \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "begin Maximum Likelihood training\n",
            "loading train data ...\n",
            "remaining train data length: 11\n",
            "pushing new round train data: 0\n",
            "geting train data ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pHqcEBZM8QA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.save(model_save_path)\n",
        "torch.save(optimizer.state_dict(), model_save_path + '.optim')\n",
        "\n",
        "ax = plt.gca()\n",
        "report_df = pd.DataFrame(train_losses)\n",
        "report_df.to_csv('train-report-64-16-10-2.csv')\n",
        "report_df.plot(kind='line',x='iter',y='loss',ax=ax)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "654noIkTRpgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax = plt.gca()\n",
        "\n",
        "reports_files = [1]\n",
        "for report in reports_files:\n",
        "  report_df = pd.read_csv('train-report-64-16-10-'+str(report)+\".csv\")  \n",
        "  report_df.plot(kind='line',x='iter',y='loss',ax=ax)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsvxsUSgnqPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(\"load test source sentences from [{}]\".format(args['TEST_SOURCE_FILE']), file=sys.stderr)\n",
        "from utils import get_voice_files_and_corpus_by_indexes\n",
        "\n",
        "voices_files = []\n",
        "test_data_tgt = []\n",
        "for voice_file, sent in get_voice_files_and_corpus_by_indexes('dataset/train', list(range(32, 48))):\n",
        "  voices_files.append(voice_file)\n",
        "  test_data_tgt.append(sent)\n",
        "\n",
        "test_data_src = load_voices_files(voices_files, 22000, 8000)\n",
        "\n",
        "\n",
        "hypotheses = beam_search(model, test_data_src,\n",
        "                          beam_size=5,\n",
        "                          max_decoding_time_step=200)\n",
        "\n",
        "top_hypotheses = [hyps[0] for hyps in hypotheses]\n",
        "bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)\n",
        "print('Corpus BLEU: {}'.format(bleu_score * 100), file=sys.stderr)\n",
        "\n",
        "with open('output2.txt', 'w') as f:\n",
        "    for src_sent, hyps in zip(test_data_src, hypotheses):\n",
        "        top_hyp = hyps[0]\n",
        "        hyp_sent = ' '.join(top_hyp.value)\n",
        "        f.write(hyp_sent + '\\n')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrerZgjNRk8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}